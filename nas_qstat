#!/usr/bin/python3 -I

'''
Display information about PBS jobs, queues, or servers

This command displays information about PBS jobs, queues, or servers,
writing the status to standard output. It is similar to OpenPBS's qstat, but
provides more options for selecting and formatting the output.
'''

# Because this program might be run as root, or with
# a random environment, check for some conditions that
# might cause security issues.


def check_perms():
    import sys
    import os
    import stat
    user = os.getuid()
    # First, clean up python search path by removing
    # dodgy entries.
    newp = []
    for x in sys.path:
        if x == '':
            continue
        try:
            sb = os.stat(x)
        except OSError:
            continue
        # Skip items not owned by root, unless
        # running as the owner of the item.
        if sb.st_uid != 0 and user != sb.st_uid:
            continue
        # Skip non-directories in path
        if not stat.S_ISDIR(sb.st_mode):
            continue
        # Skip things that others can modify
        mode = stat.S_IMODE(sb.st_mode)
        if (mode & (stat.S_IWGRP | stat.S_IWOTH)):
            continue
        if x not in newp:
            newp.append(x)
    # Update sys.path to what is left
    sys.path.clear()
    sys.path.extend(newp)

    # Now, check that we haven't already loaded
    # suspect items.
    for name, val in sys.modules.items():
        src = str(val)
        pcs = src.split()
        pathpart = pcs[-1]
        path = pathpart[1:-2]
        if path in ['built-in', 'frozen']:
            continue
        if name == '__main__':
            # For debugging, we might need to
            # add the directory where the script
            # resides back to sys.path, if
            # the -I flag removed it.
            # First, though, deal with invoked through symlink.
            t = os.path.dirname(path)
            sb = os.lstat(path)
            if stat.S_ISLNK(sb.st_mode):
                r = os.readlink(path)
                t = os.path.join(t, r)
                t = os.path.dirname(t)
            t = os.path.abspath(t)
            if t not in sys.path:
                sys.path.insert(0, t)
            # If path ends in bin, try to add equivalent lib directory
            (head,tail) = os.path.split(t)
            if tail != 'bin':
                continue
            t = os.path.join(head, 'lib')
            if t in sys.path:
                # Already there
                continue
            if not os.path.exists(t):
                continue
            # Repeat some security checks
            try:
                sb = os.lstat(t)
            except OSError:
                continue
            if not stat.S_ISDIR(sb.st_mode):
                continue
            mode = stat.S_IMODE(sb.st_mode)
            if (mode & (stat.S_IWGRP | stat.S_IWOTH)):
                continue
            # Add lib to path
            sys.path.insert(1, t)
            continue
        if "' from '" in src:
            sb = os.stat(path)
            if sb.st_uid != 0 and user != sb.st_uid:
                raise OSError("Unsafe PYTHONPATH with " + path)
            mode = stat.S_IMODE(sb.st_mode)
            if (mode & (stat.S_IWGRP | stat.S_IWOTH)):
                raise OSError("Unsafe PYTHONPATH for " + path)
    return True


if check_perms():
    # These imports are indented this way just to avoid gripes from
    # PEP-8 checkers.
    import sys
    import os
    import argparse
    import re
    import subprocess
    import signal
    import socket
    import time

    import pbs_ifl as ifl
    from nas_pbsutil import *
    import nas_layout as layout
    import nas_field_format
    from nas_field_format import *

long_desc = __doc__

args = None
gNAS = False
gNAS_job_data = {}
gNow = time.time()
gdebug = []
ghostname = ''
ghostnameshort = ''
gID = os.getuid()
gserver_info = dict()
gshare_data = dict()
gshare_entity_info = dict()
host_re = None
opts_W = list()
verbose = 0
version = "0.2"

conf.gNow = gNow
conf.pbs_conf = pbs_conf = ifl.cvar.pbs_conf


def main():
    global gNAS, verbose, args, opts_W, host_re, gdebug
    global ghostname, ghostnameshort
    # Don't catch any signals (so no ugly tracebacks on Ctrl-C).
    for i in range(1, signal.NSIG):
        try:
            signal.signal(i, signal.SIG_DFL)
        except OSError:
            pass
    prog = sys.argv[0].split('/')[-1]
    t = argparse.RawDescriptionHelpFormatter
    parser = argparse.ArgumentParser(description=long_desc, prog=prog,
                                     formatter_class=t)
    thing_type = parser.add_mutually_exclusive_group()
    display_type = parser.add_mutually_exclusive_group()
    parser.add_argument('--version', action='version',
                        version='%(prog)s ' + version)
    thing_type.add_argument('-B', default=False, action='store_true',
                            help='Display information for servers')
    parser.add_argument('-F', choices=['json', 'JSON'],
                        help='Specify encoding for -f output')
    parser.add_argument('-G', default='b', action='store_const',
                        dest='size_units', const='G',
                        help='Display sizes in gigabytes')
    parser.add_argument('-H', default=False, action='store_true',
                        help='Select only finished or moved jobs')
    parser.add_argument('-J', default=False, action='store_true',
                        help='Display status information for job arrays')
    parser.add_argument('-M', default='b', action='store_const',
                        dest='size_units', const='M',
                        help='Display sizes in 8 byte megawords')
    thing_type.add_argument('-Q', default=False, action='store_true',
                            help='Display information for queues')
    parser.add_argument('-W', action='append', metavar='display option',
                        help='Options customizing field selection and '
                        'display.')
    display_type.add_argument('-a', default=False, action='store_true',
                              help='Include information about server and '
                              'resources')
    parser.add_argument('--debug', default=[], action='append',
                        help='Debugging arguments (for developers)')
    display_type.add_argument('-f', default=0, action='count',
                              help='Full output')
    parser.add_argument('-i', default=False, action='store_true',
                        help='Select queued, held, or waiting jobs')
    parser.add_argument('-n', default=False, action='store_true',
                        help='Include exec_host information')
    parser.add_argument('-q', default=False, action='store_true',
                        help='Select alternate queue display')
    parser.add_argument('-r', default=False, action='store_true',
                        help='Select running or suspended jobs')
    parser.add_argument('-s', default=False, action='store_true',
                        help='Include job comment')
    parser.add_argument('-t', default=False, action='store_true',
                        help='Display status information for jobs, '
                        'job_arrays, and subjobs')
    parser.add_argument('-u',
                        help='Display only jobs from this user')
    parser.add_argument('--verbose', '-v', default=0, action='count',
                        help='Increase debugging verbosity')
    parser.add_argument('-x', default=False, action='store_true',
                        help='Include finished and moved jobs')
    parser.add_argument('-1', default=False, action='store_true',
                        dest='oneline',
                        help='Put any -n or -s info for a job on first line')
    parser.add_argument('things', nargs='*',
                        help='Jobs, queues, or servers to status')

    args = parser.parse_args()
    conf.verbose = verbose = args.verbose
    conf.gdebug = gdebug = ' '.join(args.debug)

    if args.F:
        args.F = args.F.lower()

    if (ifl.pbs_loadconf(0) == 0):
        print("Cannot get PBS configuration information", file=sys.stderr)
        return 1

    # Set our idea of current time, if requested.
    mo = re.search(r'fake_time=(\d+)', gdebug)
    if mo:
        try:
            gNow = int(mo.group(1))
            conf.gNow = gNow
        except ValueError:
            pass
    # Look at -B, -Q, and -a options to decide on default list of fields
    do_jobs = False
    if args.Q:
        default_fields = ['q_name', 'q_ncpus', 'q_ncpus_def', 'q_time',
                          'q_time_def', 'q_jm', 'state_count', 'q_pr',
                          'q_info']
        known_fields = gen_field_list_Q()
    elif args.B:
        default_fields = ['server_name', 'jm', 'state_count', 'default_queue',
                          'info']
        known_fields = gen_field_list_B()
    elif args.q:
        default_fields = ['q_name', 'q_memory', 'q_cput', 'q_wtime',
                          'q_nodect', 'q_jrun', 'q_jqueue', 'q_lm', 'q_state']
        known_fields = gen_field_list_q()
    else:
        do_jobs = True
        default_fields = ['jobid', 'user', 'queue', 'jobname', 'cpus', 'nds',
                          'reqdwallt', 's', 'elapwallt', 'eff']
        known_fields = gen_field_list()
        if verbose > 2:
            # Show titles of known fields
            tlist = []
            for x in known_fields:
                t = x['title']
                tlist.append(t if isinstance(t, str) else ' '.join(t))
            print('Known field titles', ', '.join(tlist))

    # Build list of known fields
    opts_W = args.W
    if opts_W is None:
        opts_W = list()
    conf.opts_W = opts_W
    # Make units for sizes available in opt_W
    opts_W.insert(0, args.size_units)
    # Get our hostname
    conf.ghostname = ghostname = socket.gethostname()
    conf.ghostnameshort = ghostnameshort = ghostname.split('.')[0]
    # Load possible user or system overrides
    more_code = load_userexits('qstat')
    if more_code:
        more_code = userexits_header + more_code
        if verbose > 2:
            print(more_code)
        prog = compile(more_code, 'userexit code', 'exec')
        exec(prog, globals(), locals())
    default_W = []
    userexit_post_opts(globals(), locals())
    opts_W[0:0] = default_W
    conf.gNAS = gNAS = check_W_bool('NAS')
    # Scan opts_W for on-the-fly field defs
    define_on_the_fly(known_fields, opts_W)
    # Set up formats
    fmtr = NAS_field_format(default_fields)
    fmtr.known_fields = known_fields
    userexit_add_fields(globals(), locals())
    # NAS allows visibility to some fields that are restricted by default.
    # Use alternate sources for the data when available.
    if not gNAS and gID != 0:
        #  Here, euser is not visible, so use Job_Owner as best substitute
        try:
            idx = [x['name'] for x in known_fields].index('user')
            known_fields[idx]['func'] = 'fmt_user_from_owner'
            known_fields[idx]['sources'] = ['Job_Owner']
        except (KeyError, ValueError):
            pass
    # Handle -W formatting options
    # First, collect list of fields of interest and which PBS attributes
    # hold the values for those fields
    (field_list, attr_list, errs) = fmtr.collect_fields()
    if (errs):
        # Check if should just print list of fields
        if field_list is None:
            print('\n'.join(errs))
            return 0
        print(errs, file=sys.stderr)
        if len(field_list) == 0:
            return 1
    # Adjust formatting of selected fields, per -W fmt_xxx values
    fmtr.field_list = field_list
    rc = fmtr.adjust_formats(field_list)
    if rc is not True:
        print(rc, file=sys.stderr)
        return 1
    # Set formatting globals from cmd line args
    nas_field_format.set_field_vars(opts_W)
    # Decide if will be filtering based on execution host
    host_patt = check_W_str('host')
    if host_patt is None:
        host_patt = check_W_str('hosts')
    # Convert host list in form aaa,bbb,ccc to pattern alternative aaa|bbb|ccc
    # and compile it.
    if host_patt:
        host_re = re.compile(r'\b(' + host_patt.replace(',', '|') + r')\b',
                             re.I)
    else:
        host_re = None
    if do_jobs and host_re:
        # If we are filtering by host, make sure we ask for exec_host
        attr_list.add('exec_host')
    # Similarly, -n requires exec_host and -s requires comment
    if do_jobs and args.n:
        attr_list.add('exec_host')
    if do_jobs and args.s:
        attr_list.add('comment')
    # If filtering by user/job, make sure we have those
    if do_jobs and check_W_str('u'):
        attr_list.add('Job_Owner')
        attr_list.add('Job_Name')
    # Build list of attributes we need to ask for.
    attr_list = (','.join(attr_list)).split(',')
    while '' in attr_list:
        attr_list.remove('')
    if args.f == 1:
        attr_list = list()
    if verbose > 2:
        print('Requested attribute list:', ', '.join(attr_list))
    # An empty attribute list is rejected. For servers and queues, just
    # ask for everything. For jobs, ask for something easy.
    if len(attr_list) > 0:
        atl = list_to_attrl(attr_list)
    else:
        if args.f:
            atl = None
        elif do_jobs:
            t = ['job_state']
            atl = list_to_attrl(t)
        else:
            atl = None
    fmtr.atl = atl

    # Last, get and display requested information
    if args.Q or args.q:
        rc = display_queues(args, fmtr)
    elif args.B:
        rc = display_servers(args, fmtr)
    else:
        rc = display_jobs(args, fmtr)
    return rc


def display_jobs(args, fmtr):
    '''Output job info

    For each job, generate a list of display values for each
    selected field. Collect those lists into a list and pass it to
    layout for display.
    Args:
        args = result from argparse of command line
        fmtr = field formatter object
    Returns: 0 on success
    '''
    global opts_W, host_re, gdebug

    # Examine options to set selection and extend attributes
    selects = dict()
    extend = set()
    if args.i:
        selects['job_state'] = 'HQTW'
    if args.r or host_re:
        selects['job_state'] = 'BERSU'
    if args.H:
        selects['job_state'] = 'FMX'
        extend.add('x')
    if args.t:
        extend.add('t')
    if args.x:
        extend.add('x')
    if args.u:
        selects['User_List'] = args.u
    if args.J:
        selects['array'] = 'True'
    extend = ''.join(extend)

    alt_disp = True if args.a else False

    # Build base layout
    cfg = layout.Config()
    for fld in fmtr.field_list:
        title = fld['title']
        if not title:
            title = fld['name']
        cfg.add_field(title, ident=fld['name'], **fld['format'])
    # Gather data from server
    server_conn = {'': None, None: None}
    sel_attr = dict_to_attropl(selects, ifl.EQ)
    things = args.things
    errcnt = 0
    if len(things) == 0:
        # No destinations, ask for jobs at default server
        things.append('@' + pbs_conf.pbs_server_name)
    # There can be three types of objects to get job info for:
    # individual jobs, queues, and servers. Distinguish them
    info = []
    idx = 0
    while idx < len(things):
        thing = things[idx].strip()
        idx += 1
        seq_num = None
        is_jobs = False
        if thing[0] not in '123456789':
            # Server or queue
            if thing[0] == '@':
                queue = None
                names = [thing]
                current_server = thing[1:]
            else:
                t = thing.split('@')
                queue = t[0]
                names = [queue]
                current_server = t[1] if len(t) > 1 else None
        else:
            is_jobs = True
            # Job id
            try:
                (seq_num, parent_server, current_server, array_idx, _) = \
                    parse_jobid(thing)
            except Exception:
                print("Unrecognized destination: %s" % thing,
                      file=sys.stderr)
                errcnt += 1
                continue
            # Collect sequence of jobids at same server into one list.
            names = []
            jobid = seq_num
            if not parent_server:
                if current_server:
                    parent_server = current_server
                else:
                    parent_server = pbs_conf.pbs_server_name
            if array_idx:
                jobid += array_idx
            jobid += '.' + parent_server
            add_server_conn(server_conn, parent_server, None)
            if current_server and current_server not in server_conn:
                conn = ifl.pbs_connect(current_server)
                add_server_conn(server_conn, current_server, conn)
            names.append(jobid)
            prev_conn = server_conn[current_server]
            # Examine next entry to see if more jobids
            while idx < len(things):
                peek = things[idx]
                if peek[0] not in '123456789':
                    break
                try:
                    (s, p, c, a, _) = parse_jobid(peek)
                except Exception:
                    c = None
                # Detect change of server
                if server_conn.get(c, -2) != prev_conn:
                    break
                jobid = s
                if a:
                    jobid += a
                if not p:
                    if c:
                        p = current_server
                    else:
                        p = pbs_conf.pbs_server_name
                jobid += '.' + p
                names.append(jobid)
                idx += 1
        # Now, issue query to current_server
        if current_server is None:
            current_server = pbs_conf.pbs_server_name
        conn = server_conn.get(current_server, None)
        if conn is None:
            conn = ifl.pbs_connect(current_server)
            add_server_conn(server_conn, current_server, conn)
            if conn < 0:
                print("Cannot connect to PBS server %s: %s" %
                      (current_server, os.strerror(ifl.get_pbs_errno())),
                      file=sys.stderr)
                errcnt += 1
                continue
        if conn == -1:
            # Known bad server name, skip
            continue
        # If running at NAS, cache share & priority info
        if gNAS:
            cache_shares(current_server)
            extract_share_info(current_server)
            extract_job_info(current_server)
        set_entity_map(gshare_entity_info)
        # Give userexits a chance to adjust things based on server
        userexit_set_server(globals(), locals())
        # Display server header, if requested
        if alt_disp:
            cache_server(current_server, conn)
            display_server_hdr(current_server, conn, args, opts_W)
        # Get info for selected jobs
        sname = current_server.split('.')[0]
        skip_jobs = check_W_bool('skip_jobs')
        if skip_jobs:
            bs = []
        else:
            bs = file_to_stat(sname, 'jobs', fmtr.atl)
        if bs is None:
            namelist = ','.join(names)
            if sel_attr:
                bs = ifl.pbs_selstat(conn, sel_attr, fmtr.atl, extend)
            else:
                bs = ifl.pbs_statjob(conn, namelist, fmtr.atl, extend)
            err = ifl.get_pbs_errno()
            if err:
                errcnt += 1
                errmsg = ifl.pbs_geterrmsg(conn)
                if errmsg is None or errmsg == '':
                    errmsg = "error %d" % err
                errmsg += ': ' + namelist
                print(errmsg, file=sys.stderr)
                continue
        if gNAS:
            plug_job_info(bs)
        # Give userexits a chance to tweak info for jobs
        userexit_post_statjob(globals(), locals())
        if gNAS and check_W_int('shares', 0):
            display_shares(gshare_entity_info)
        # If interested only in jobs on given host, filter the list
        if host_re:
            newbs = []
            for job in bs:
                hosts = job.get('exec_host')
                if not hosts:
                    continue
                if not host_re.search(hosts):
                    continue
                newbs.append(job)
            bs = newbs
        # If interested only in specific user jobs, filter the list
        ulist = check_W_str('u')
        if ulist:
            patts = ulist.split(',')
            patt_re = []
            for t in patts:
                if '#' not in t:
                    t = t + '#.*'
                # Convert negated patterns to negative look-ahead
                if t.startswith('!'):
                    t = '(?!' + t[1:] + ').*'
                try:
                    r = re.compile(t + '$')
                except Exception:
                    continue
                patt_re.append(r)
            newbs = []
            for job in bs:
                owner = job.get('Job_Owner', '').split('@')[0]
                jobname = job.get('Job_Name', '')
                for r in patt_re:
                    if r.match(owner + '#' + jobname):
                        newbs.append(job)
                        break
            bs = newbs
        if sel_attr and names and is_jobs:
            # You cannot specify a list of names to the pbs_selstat()
            # call, so we have to filter the results ourselves when
            # explicit request IDs were given.
            newbs = []
            name_set = set(names)
            found = set()
            for job in bs:
                jobid = job['id']
                if jobid in name_set:
                    found.add(jobid)
                    newbs.append(job)
            bs = newbs
            missing = name_set.difference(found)
            if missing:
                print("Excluded Job Id%s:" %
                      ('' if len(missing) == 1 else 's'),
                      ' '.join(missing),
                      file=sys.stderr)
        if bs:
            info.extend(bs)
        names = []
    # End of idx loop
    # Close server connections
    for conn in set(server_conn.values()):
        if conn is not None and conn != -1:
            ifl.pbs_disconnect(conn)
    if args.f:
        t = "Job" if conf.gNAS else "Job Id"
        return display_f(args, info, t, "Jobs")
    # Tag sort job list, unless -W do_sort=false

    def cmpf(i):
        def safeint(x):
            try:
                t = int(x)
            except ValueError:
                t = 0
            return t
        job = info[i]
        st = job.get('job_state', '?')
        k1 = 'BRFQHWTEX'.find(st)
        if st in 'BRF':
            k2 = safeint(job.get('Priority', '0'))
            k3 = job.get('queue', '')
            k4 = safeint(job.get('Resource_List.nodect', '0'))
            k4 = safeint(job.get('Resource_List.ncpus', '0'))
            k5 = clocktosecs(job.get('Resource_List.walltime', ''))
            if not isinstance(k5, int):
                k5 = 0
            return (k1, -k2, k3, -k4, -k5, i)
        elif st in 'Q':
            k2 = safeint(job.get('spri', '999999'))
            k3 = safeint(job.get('Priority', '0'))
            k4 = safeint(job.get('Resource_List.nodect', '0'))
            k5 = clocktosecs(job.get('Resource_List.walltime', ''))
            if not isinstance(k5, int):
                k5 = 0
            (s, p, _, a, _) = parse_jobid(job.get('id', '1.unknown'))
            k6 = p
            k7 = safeint(s)
            k8 = safeint(a.strip('[]') if a else '0')
            return (k1, k2, -k3, -k4, -k5, k6, k7, k8, i)
        (s, p, _, a, _) = parse_jobid(job.get('id', '1.unknown'))
        k2 = p
        k3 = safeint(s)
        k4 = safeint(a.strip('[]') if a else '0')
        return (k1, k2, k3, k4, i)
    if check_W_bool('do_sort', True):
        tags = sorted(range(len(info)), key=cmpf)
    else:
        tags = range(len(info))
    # Display info
    rows = []
    for jobi in tags:
        job = info[jobi]
        row = list()
        for f in fmtr.field_list:
            func = getattr(nas_field_format, f['func'])
            row.append(func(f, job))
        rows.append(row)
    endl = '' if args.oneline else '\n'
    indent = '' if args.oneline else '  '
    if rows:
        new_section()
        show_hdr = '-h' not in opts_W and 'noheader' not in opts_W
        lines = layout.layout(cfg, rows, show_hdr=show_hdr)
        hdr_count = len(lines) - len(rows)
        i = 0
        for line in lines:
            print(line.rstrip(), end=endl)
            jobi = i - hdr_count
            if jobi >= 0 and args.n:
                t = info[tags[jobi]].get('exec_host', None)
                if t:
                    print(indent, t, end=endl)
            if jobi >= 0 and args.s:
                t = info[tags[jobi]].get('comment', None)
                if t:
                    if len(t) > 76:
                        t = t[:73] + '...'
                    print(indent, t, end=endl)
            if args.oneline:
                print()
            i += 1
    return 1 if errcnt else 0


def display_queues(args, fmtr):
    '''Output info about queues

    For each queue, generate a list of display values for each
    selected field. Collect those lists into a list and pass it to
    layout for display.
    Args:
        args = result from argparse of command line
        fmtr = field formatter object
    Returns: 0 on success
    Note that the -q version of the display adds several lines to each
    server's display.
    '''
    global opts_W

    cfg = layout.Config()
    state_count_idx = -1
    for idx, fld in enumerate(fmtr.field_list):
        title = fld['title']
        if not title:
            title = fld['name']
        cfg.add_field(title, ident=fld['name'], **fld['format'])
        if fld['func'] == 'fmt_state_count':
            state_count_idx = idx
    sinfo = []

    svr_present = False
    # Get info about queues
    if args.things is None or len(args.things) == 0:
        sname = pbs_conf.pbs_server_name.split('.')[0]
        bs = file_to_stat(sname, 'queues')
        if bs is None:
            conn = ifl.pbs_connect(sname)
            if conn < 0:
                print("Cannot connect to PBS server: %s" %
                      os.strerror(ifl.get_pbs_errno()),
                      file=sys.stderr)
                return 1
            bs = ifl.pbs_statque(conn, '', fmtr.atl, None)
            ifl.pbs_disconnect(conn)
        sinfo.append((sname.split('.')[0], bs))
    else:
        prev = None
        conn = -1
        for thing in args.things:
            parts = thing.split('@')
            if len(parts) < 2:
                parts.append('')
            else:
                svr_present = True
            (qname, sname) = parts[0:2]
            if sname == '':
                sname = pbs_conf.pbs_server_name.split('.')[0]
            new_svr = False
            if sname != prev:
                new_svr = True
                if conn != -1:
                    ifl.pbs_disconnect(conn)
                conn = ifl.pbs_connect(sname)
                if conn == -1:
                    print("Cannot connect to PBS server %s: %s" %
                          (sname, os.strerror(ifl.get_pbs_errno())),
                          file=sys.stderr)
                    prev = None
                    continue
                prev = sname
            bs = file_to_stat(sname, 'queues')
            if bs is None:
                bs = ifl.pbs_statque(conn, qname, fmtr.atl, None)
            if len(bs) == 0:
                continue
            if new_svr:
                sinfo.append((sname.split('.')[0], bs))
            else:
                sinfo[-1][1].extend(bs)
        if conn != -1:
            ifl.pbs_disconnect(conn)
    if args.f:
        tbs = []
        for (sname, info) in sinfo:
            tbs.extend(info)
        return display_f(args, tbs, "Queue", "Queue")
    # Deal with state_count field by patching field title and
    # adding 'pretty_sc' attribute to each queue
    if state_count_idx >= 0:
        for (sname, qinfo) in sinfo:
            scounts = pretty_state_counts(qinfo)
            fs = cfg.config[state_count_idx]
            fs.title[1] = scounts.pop(0)
            for idx, que in enumerate(qinfo):
                que['pretty_sc'] = scounts[idx]
    # For -q output, compute queued and running counts
    if args.q:
        for (sname, qinfo) in sinfo:
            for que in qinfo:
                t = que.get('state_count', None)
                if t is None:
                    continue
                (queued, running) = decode_state_count(t)
                que['queued'] = queued
                que['running'] = running
    # Format selected fields and use layout to present them
    for (sname, qinfo) in sinfo:
        rows = []
        if args.q:
            for f in fmtr.field_list:
                if 'sum' in f['opt']:
                    f['total'] = 0
        for que in qinfo:
            row = list()
            for f in fmtr.field_list:
                func = getattr(nas_field_format, f['func'])
                t = func(f, que)
                row.append(t)
                if args.q:
                    if 'sum' in f['opt']:
                        try:
                            f['total'] += int(t)
                        except ValueError:
                            pass
            rows.append(row)
        if rows:
            new_section()
            show_hdr = '-h' not in opts_W and 'noheader' not in opts_W
            if show_hdr and svr_present:
                print("\nserver: %s\n" % sname)
                if args.q:
                    # Add job count sum lines
                    dsh_row = []
                    tot_row = []
                    for f in fmtr.field_list:
                        if 'sum' in f['opt']:
                            t = str(f['total'])
                            dsh_row.append('-' * max(3, len(t)))
                            tot_row.append(t)
                        else:
                            dsh_row.append('')
                            tot_row.append('')
                    rows.append(dsh_row)
                    rows.append(tot_row)
            res = layout.layout(cfg, rows, show_hdr=show_hdr)
            print('\n'.join([x.rstrip() for x in res]))
    return 0


def display_servers(args, fmtr):
    '''Output info about servers

    For each server, generate a list of display values for each
    selected field. Collect those lists into a list and pass it to
    layout for display.
    Args:
        args = result from argparse of command line
        fmtr = field formatter object
    Returns: 0 on success
    '''
    global opts_W

    cfg = layout.Config()
    idx = 0
    state_count_idx = -1
    for fld in fmtr.field_list:
        title = fld['title']
        if not title:
            title = fld['name']
        cfg.add_field(title, ident=fld['name'], **fld['format'])
        if fld['func'] == 'fmt_state_count':
            state_count_idx = idx
        idx += 1
    rows = []
    info = []
    # Get info about servers
    if args.things is None or len(args.things) == 0:
        conn = ifl.pbs_connect('')
        if conn < 0:
            print("Cannot connect to PBS server: %s" %
                  os.strerror(ifl.get_pbs_errno()),
                  file=sys.stderr)
            return 1
        bs = ifl.pbs_statserver(conn, fmtr.atl, None)
        ifl.pbs_disconnect(conn)
        info.extend(bs)
    else:
        prev = None
        conn = -1
        for thing in args.things:
            if thing != prev:
                if conn != -1:
                    ifl.pbs_disconnect(conn)
                conn = ifl.pbs_connect(thing)
                if conn == -1:
                    print("Cannot connect to PBS server %s: %s" %
                          (thing, os.strerror(ifl.get_pbs_errno())),
                          file=sys.stderr)
                    prev = None
                    continue
                prev = thing
            bs = ifl.pbs_statserver(conn, fmtr.atl, None)
            if len(bs) == 0:
                continue
            info.extend(bs)
        if conn != -1:
            ifl.pbs_disconnect(conn)
    if args.f:
        if info:
            new_section()
        return display_f(args, info, "Server", "Server")
    # Deal with state_count field by patching field title and
    # adding 'pretty_sc' attribute to each server
    if state_count_idx >= 0:
        scounts = pretty_state_counts(info)
        fs = cfg.config[state_count_idx]
        fs.title[1] = scounts.pop(0)
        for idx, svr in enumerate(info):
            svr['pretty_sc'] = scounts[idx]
    for svr in info:
        row = list()
        for f in fmtr.field_list:
            func = getattr(nas_field_format, f['func'])
            row.append(func(f, svr))
        rows.append(row)
    if rows:
        new_section()
        show_hdr = '-h' not in opts_W and 'noheader' not in opts_W
        res = layout.layout(cfg, rows, show_hdr=show_hdr)
        print('\n'.join([x.rstrip() for x in res]))
    return 0


def cache_server(server, conn):
    '''Cache info about a server

    Args:
        server = server name
        conn = PBS connection to server
    Exit:
        Info about server saved in gserver_info hash
    '''
    global gserver_info, gdebug
    sname = server.split('.')[0]
    if sname in gserver_info:
        return gserver_info[sname]
    bs = file_to_stat(sname, 'server')
    if bs is None:
        bs = ifl.pbs_statserver(conn, None, None)
    info = bs[0]
    gserver_info[sname] = info
    if info is None:
        return info
    # Get reservation list
    atrs = ['reserve_end', 'reserve_start']
    resv_info = file_to_stat(sname, 'resvs', atrs)
    if resv_info is None:
        atrl = list_to_attrl(atrs)
        resv_info = ifl.pbs_statresv(conn, None, atrl, None)
    info['resv_info'] = resv_info
    return info


def cache_shares(server):
    '''Cache share info for server

    Args:
        server = server name
    '''
    global gshare_data, ghostname, ghostnameshort
    sname = server.split('.')[0]
    if sname in gshare_data:
        # Already cached, done
        return
    # Try various places to locate file
    pbshome = pbs_conf.pbs_home_path
    syspath = os.path.join(pbshome, 'sched_priv', 'sortedjobs')
    if sname == ghostnameshort:
        # Running on server host, get file directly
        try:
            with open(syspath) as f:
                data = f.read()
        except Exception:
            data = None
    else:
        # Look for the file in various places
        for d in ('/home1', '/home', '/u'):
            fname = os.path.join(d, 'pbs_status', sname)
            if os.path.exists(fname):
                break
            fname = os.path.join(d, 'pbs_status', server)
            if os.path.exists(fname):
                break
        else:
            fname = None
        if fname:
            try:
                with open(fname) as f:
                    data = f.read()
            except Exception:
                data = None
        else:
            data = None
    # Cache whatever results we have
    gshare_data[server] = data
    if server != sname:
        gshare_data[sname] = data


def display_server_hdr(server, conn, args, opts_W):
    '''Display info about server

    Show summary information about a server (before the display
    of jobs on that server).

    Args:
        server = server name
        conn = connection index of server
        args = parsed command line arguments
        opts_W = -W options
    '''
    def safeint(x):
        try:
            t = int(x)
        except ValueError:
            t = 0
        return t
    sname = server.split('.')[0]
    info = gserver_info[sname]
    new_section()
    in_server_header = True
    current_server = info.get('id', server)
    show_hdr = '-h' not in opts_W and 'noheader' not in opts_W
    if show_hdr:
        print(current_server + ':    ', time.asctime(time.localtime()))
    # Create one-line job header
    total_jobs = safeint(info.get('total_jobs', '0'))
    counts = info.get('state_count', '')
    comment = info.get('comment')
    scheduling = info.get('scheduling')
    clist = []
    jcnt = 0
    for x in counts.split():
        y = x.split(':')
        if len(y) != 2:
            continue
        clist.append(y[0][0] + ':' + y[1])
        jcnt += safeint(y[1])
    done_count = total_jobs - jcnt
    if done_count > 0 and args.x:
        done_string = " + %d finished" % done_count
    else:
        done_string = ''
    plural = 's' if jcnt != 1 else ''
    if show_hdr:
        print(" Server reports %d job%s total (%s)%s" % (jcnt, plural,
              ' '.join(clist), done_string))
        # Add any server comment and scheduler status
        if comment:
            print("\t", comment)
        if scheduling != 'True':
            print("\t+++ Scheduling turned off.")
    # Display info about MoMs
    all_resvs = info.get('resv_info', ())
    fmta_init(all_resvs, userexit_interest)
    node_detail = check_W_bool('node_detail')
    default_fields = ['host', 'cpus', 'gpus', 'tasks', 'jobs', 'ninfo']
    if node_detail:
        default_fields = ['host', 'cused', 'cfree', 'gused', 'gfree', 'mused',
                          'mfree', 'tasks', 'jobs', 'ninfo']
    fmtr = NAS_field_format(default_fields, 'fmta')
    known_fields = gen_field_list_a()
    define_on_the_fly(known_fields, opts_W, 'oa')
    fmtr.known_fields = known_fields
    userexit_add_fields_a(globals(), locals())
    (field_list, attr_set, errs) = fmtr.collect_fields('oa')
    if (errs):
        # Check if should just print list of fields
        if field_list is None:
            print('\n'.join(errs))
            return 0
        print(errs, file=sys.stderr)
    if check_W_bool('node_comments', False):
        attr_set.add('comment')
    if not check_W_bool('node_comments', True):
        attr_set.discard('comment')
    if check_W_str('condense_vnodes'):
        attr_set.add('resources_available')
    # Adjust formatting of selected fields, per -W fmta_xxx values
    fmtr.field_list = field_list
    rc = fmtr.adjust_formats(field_list)
    if rc is not True:
        print(rc, file=sys.stderr)
        return 1
    nas_field_format.set_field_vars(opts_W)
    cfg = layout.Config()
    ninfo_idx = -1
    idx = 0
    for fld in field_list:
        title = fld['title']
        if not title:
            title = fld['name']
        cfg.add_field(title, ident=fld['name'], **fld['format'])
        if fld['name'] == 'ninfo':
            ninfo_idx = idx
        idx += 1
    # If server is in promising state, collect info about each MoM
    mom_info = info.get('mom_info', None)
    if (mom_info is None):
        state = info.get('server_state', '')
        if re.search(r'Scheduling|Active|Idle', state):
            attr_list = (','.join(attr_set)).split(',')
            while '' in attr_list:
                attr_list.remove('')
            atl = None if len(attr_list) == 0 else list_to_attrl(attr_list)
            mom_info = file_to_stat(sname, 'vnodes', attr_list)
            if mom_info is None:
                if conn is not None:
                    mom_info = ifl.pbs_statvnode(conn, None, atl, None)
            # Condense vnode info into natural vnodes if desired
            cv = check_W_str('condense_vnodes', '')
            cvhosts = re.split(r'[\s,]+', cv)
            if sname in cvhosts or check_W_bool('condense_vnodes'):
                mom_info = condense_vnode_info(mom_info)
        info['mom_info'] = mom_info
    mom_list = []
    for minfo in mom_info:
        mom_name = minfo.get('id')
        if not mom_name:
            continue
        # If interested in specific hosts, skip those which match neither
        # the mom id nor the host name.
        if host_re:
            if not host_re.match(mom_name):
                if not host_re.match(minfo.get('host', '')):
                    continue
        mom_row = list()
        for f in fmtr.field_list:
            func = getattr(nas_field_format, f['func'])
            mom_row.append(func(f, minfo))
        mom_list.append(mom_row)
    userexit_set_server(globals(), locals())
    # Handle summarizing
    nrows = mom_list
    t = check_W_int('node_bin_total', 10)
    if t > 0 and len(nrows) >= t and ninfo_idx >= 0:
        # sort by minfo field
        nrows.sort(key=lambda mom: mom[ninfo_idx])
        node_bin_size = check_W_int('node_bin_size', 2)
        new_rows = []
        start = 0
        match = nrows[start][ninfo_idx]
        # Scan sorted list, collecting nodes with similar info fields.
        # Use dummy last entry to force out final real entries
        for idx in range(len(nrows) + 1):
            mom_row = nrows[idx] if idx < len(nrows) \
                else (['NO_MATCH'] * (ninfo_idx + 1))
            info = mom_row[ninfo_idx]
            if info != match:
                count = idx - start
                if count <= node_bin_size:
                    # If fewer similar nodes than node_bin_size, include nodes
                    # directly
                    new_rows.extend(nrows[start:idx])
                else:
                    # Otherwise, summarize nodes into one entry
                    sum_row = summarize_nodes(field_list, nrows[start:idx])
                    new_rows.append(sum_row)
                match = info
                start = idx
        nrows = new_rows
    # Give admin/user one last chance to tweak field values before layout
    userexit_last_chance_a(globals(), locals())
    # Layout result
    if len(nrows) > 0:
        if show_hdr:
            new_section()
        rows = layout.layout(cfg, nrows, show_hdr=show_hdr)
        for r in rows:
            indent = ' ' if show_hdr else ''
            print(indent, r.rstrip(), sep='')


def display_shares(emap):
    '''Display NAS mission share info

    Args:
        emap = Cached share entity info from sortedjobs file
    '''
    share_opts = check_W_int('shares')
    slist = list()
    # Collect some info from root entry
    item = emap.get('root', None)
    if not item:
        return
    totalalloc = item['gross']
    totalncpus = item['alloc']
    # Visit the saved share data in the original order
    for entity in sorted(emap, key=lambda e: emap[e]['idx']):
        item = emap[entity]
        gross = item['gross']
        net = item['net']
        alloc = item['alloc']
        demand = item.get('demand', 0)
        if gross == -1:
            continue
        if gross == 0 and demand == 0:
            continue
        if totalalloc > 0:
            sharepct = gross * 100 // totalalloc
        else:
            sharepct = 0
        (exempt, limited, borrow) = [item[x] for x in ('e', 'l', 'b')]
        demand = item['ed'] + item['ld'] + item['bd']
        # Skip entries with nothing to offer
        if gross == 0 and demand == 0:
            continue
        shareuse = limited + borrow
        if totalncpus > 0:
            usepct_str = str(shareuse * 100 // totalncpus)
        else:
            usepct_str = '--'
        avail = alloc - shareuse
        if avail < 0:
            borrowed = shareuse - alloc
            avail = 0
        else:
            borrowed = 0
        if alloc > 0:
            t = float(shareuse) / float(alloc)
            ratio_str = '%.2f' % t
        else:
            ratio_str = '--'
        if share_opts & 2:
            # TODO
            backlog = 0
            backlog_str = secstoclock(backlog, False, check_W_bool('human'))
        if entity == 'root':
            entity_str = 'Overall'
        else:
            entity_str = ' ' + entity
        sinfo = [entity_str, str(sharepct), usepct_str, str(alloc),
                 str(exempt), str(shareuse), str(avail),
                 ratio_str, str(demand)]
        if share_opts & 2:
            sinfo.append(backlog_str)
        slist.append(sinfo)
    # Unlikely, but there might be nothing to display
    if len(slist) == 0:
        return
    # Construct formatter
    c = layout.Config()
    c.add_field('Group')
    c.add_field('Share%', hj='r')
    c.add_field('Use%', hj='r')
    c.add_field('Share', hj='r')
    c.add_field('Exempt', hj='r')
    c.add_field('Use', hj='r')
    c.add_field('Avail', hj='r')
    c.add_field('Ratio', hj='r')
    c.add_field('Waiting', hj='r')
    if share_opts & 2:
        c.add_field('Backlog', hj='r')
    # Format and print info
    new_section()
    show_hdr = '-h' not in opts_W and 'noheader' not in opts_W
    rows = layout.layout(c, slist, 0, show_hdr)
    print('\n'.join([x.rstrip() for x in rows]))
    return


def gen_field_list():
    '''Generate list of known fields for jobs

    Returns: list of field_info dicts for known fields
    The dicts have these key/value pairs
        name: the name of the field
        format: dict suitable for passing to layout.add_field()
        func: name of function to calculate display value
        sources: reservation attributes whose values are needed by func
    '''
    fl = []
    rj = {'hj': 'r'}
    hlrj = {'hj': 'l', 'rj': 'r'}
    fl.append(gen_field('acct', 'Acct', None, 'fmt_by_attr', 'Account_Name'))
    fl.append(gen_field('aoe', 'AOE', None, 'fmt_aoe', 'schedselect'))
    fl.append(gen_field('comment', 'Comment', None, 'fmt_by_attr', 'comment'))
    fl.append(gen_field('cnt', 'Cnt', rj, 'fmt_by_attr', None))
    fl.append(gen_field('cpct', 'Cpct', rj, 'fmt_from_rsrc',
                        'resources_used', 'cpupercent'))
    fl.append(gen_field('cpus', 'CPUs', rj, 'fmt_from_rsrc',
                        'Resource_List', 'ncpus'))
    fl.append(gen_field('cput', 'Cput', rj, 'fmt_from_rsrc',
                        'resources_used', 'cput'))
    fl.append(gen_field('ctime', 'Ctime', None, 'fmt_date', 'ctime'))
    fl.append(gen_field('eff', 'Eff', rj, 'fmt_efficiency', 'resources_used'))
    fl.append(gen_field('elapwallt', ['Elap', 'wallt'], rj, 'fmt_elapsed',
                        'resources_used etime job_state'))
    fl.append(gen_field('eligstart', ['Eligible', 'start'], hlrj, 'fmt_date',
                        'etime'))
    fl.append(gen_field('eligtime', ['Elig', 'time'], rj, 'fmt_elig_time',
                        'eligible_time'))
    fl.append(gen_field('estend', ['Est', 'end'], None, 'fmt_est_end',
                        'job_state stime estimated Resource_List '
                        'resources_used'))
    fl.append(gen_field('eststart', ['Est', 'start'], rj, 'fmt_future_date',
                        'estimated', 'start_time'))
    fl.append(gen_field('exechost', 'Exec_host', None, 'fmt_by_attr',
                        'exec_host'))
    fl.append(gen_field('exitstatus', ['Exit', 'status'], rj, 'fmt_by_attr',
                        'Exit_status'))
    fl.append(gen_field('gpus', 'GPUs', rj, 'fmt_from_rsrc',
                        'Resource_List', 'ngpus'))
    fl.append(gen_field('group', 'Group', None, 'fmt_by_attr', 'egroup'))
    fl.append(gen_field('jobid', 'JobID', None, 'fmt_jobid', None))
    fl.append(gen_field('jobname', 'Jobname', None, 'fmt_by_attr', 'Job_Name'))
    fl.append(gen_field('lifetime', ['Life', 'time'], rj, 'fmt_lifetime',
                        'job_state qtime mtime'))
    fl.append(gen_field('maxwallt', ['Max', 'wallt'], rj, 'fmt_from_rsrc_tm',
                        'Resource_List', 'max_walltime'))
    fl.append(gen_field('memory', 'Memory', hlrj, 'fmt_from_rsrc_sz',
                        'resources_used Resource_List', 'mem'))
    fl.append(gen_field('minwallt', ['Min', 'wallt'], rj, 'fmt_from_rsrc_tm',
                        'Resource_List', 'min_walltime'))
    fl.append(gen_field('mission', 'Mission', None, 'fmt_mission', 'egroup euser Priority'))
    fl.append(gen_field('model', 'Model', None, 'fmt_model', 'schedselect'))
    fl.append(gen_field('nds', 'Nds', rj, 'fmt_from_rsrc',
                        'resources_used Resource_List', 'nodect'))
    fl.append(gen_field('place', 'Place', None, 'fmt_by_name',
                        'Resource_List'))
    fl.append(gen_field('pmem', 'Pmem', hlrj, 'fmt_from_rsrc',
                        'resources_used', 'mem'))
    fl.append(gen_field('pri', 'Pri', rj, 'fmt_by_attr', 'Priority'))
    fl.append(gen_field('qtime', 'Qtime', None, 'fmt_date_full', 'qtime'))
    fl.append(gen_field('queue', 'Queue', None, 'fmt_by_attr', 'queue'))
    fl.append(gen_field('rank0', 'Rank0', None, 'fmt_rank0', 'exec_host'))
    fl.append(gen_field('reqid', 'ReqID', None, 'fmt_full_id', None))
    fl.append(gen_field('reqmem', 'Reqmem', hlrj, 'fmt_from_rsrc_sz',
                        'Resource_List', 'mem'))
    fl.append(gen_field('remwallt', ['Rem', 'wallt'], rj, 'fmt_remaining',
                        'Resource_List resources_used job_state'))
    fl.append(gen_field('reqdwallt', ['Req\'d', 'wallt'], rj,
                        'fmt_from_rsrc_tm', 'Resource_List', 'walltime'))
    fl.append(gen_field('runs', 'Runs', rj, 'fmt_by_attr', 'run_count'))
    fl.append(gen_field('s', 'S', None, 'fmt_by_attr', 'job_state'))
    fl.append(gen_field('sessid', 'SessID', rj, 'fmt_by_attr', 'session_id'))
    fl.append(gen_field('seqno', 'SeqNo', None, 'fmt_seqno', None))
    fl.append(gen_field('ss', 'Ss', None, 'fmt_jobstate',
                        'job_state Hold_Types'))
    fl.append(gen_field('stime', 'Stime', None, 'fmt_date', 'stime'))
    fl.append(gen_field('user', 'User', None, 'fmt_by_attr', 'euser'))
    fl.append(gen_field('vmem', 'Vmem', hlrj, 'fmt_from_rsrc_sz',
                        'resources_used', 'vmem'))
    return fl


def gen_field_list_B():
    '''Generate field list for server display
    '''
    fl = []
    dr = {'dj': 'r'}
    de = {'df': '='}
    dre = {'dj': 'r', 'df': '='}
    fl.append(gen_field('server_name', ['Server', 'name'], de, 'fmt_id', None))
    fl.append(gen_field('jm', 'jm', dre, 'fmt_by_attr', 'max_running'))
    fl.append(gen_field('state_count', ['State count', ''], de,
                        'fmt_state_count', 'state_count'))
    fl.append(gen_field('default_queue', ['Default', 'queue'], de,
                        'fmt_by_attr', 'default_queue'))
    fl.append(gen_field('info', 'Info', de, 'fmt_server_info',
                        'server_state comment scheduling'))
    return fl


def gen_field_list_Q():
    '''Generate field list for queue display
    '''
    fl = []
    rr = {'rj': 'r'}
    de = {'df': '='}
    dre = {'rj': 'r', 'df': '='}
    dle = {'rj': 'l', 'df': '='}
    dfltf = {'hj': 'r', 'hs': '/', 'df': '='}
    fl.append(gen_field('q_name', ['Queue', 'name'], de, 'fmt_id', None))
    fl.append(gen_field('q_ncpus', ['Ncpus', 'max'], dfltf, 'fmt_from_rsrc',
                        'resources_max', 'ncpus'))
    fl.append(gen_field('q_ncpus_def', 'def', dle, 'fmt_from_rsrc',
                        'resources_default', 'ncpus'))
    fl.append(gen_field('q_time', ['Time', 'max'], dfltf, 'fmt_from_rsrc_tm',
                        'resources_max', 'walltime'))
    fl.append(gen_field('q_time_def', 'def', dle, 'fmt_from_rsrc_tm',
                        'resources_default', 'walltime'))
    fl.append(gen_field('q_jm', 'jm', dre, 'fmt_by_attr', 'max_running'))
    fl.append(gen_field('state_count', ['State count', ''], de,
                        'fmt_state_count', 'state_count'))
    fl.append(gen_field('q_pr', 'pr', dre, 'fmt_by_attr', 'Priority'))
    fl.append(gen_field('q_info', 'Info', de, 'fmt_queue_info',
                        'enabled started hasnodes comment'))
    return fl


def gen_field_list_a():
    '''Generate field list for -a node display
    '''
    fl = []
    fl.append(gen_field('host', 'Host', {'rj': 'lr'}, 'fmta_host', None,
                        'sum_hosts'))
    fl.append(gen_field('cpus', 'CPUs', {'hj': 'r'}, 'fmta_count',
                        'resources_available', 'ncpus sum'))
    fl.append(gen_field('cused', ['CPUs', 'used'], {'hj': 'r', 'hs': '/'},
                        'fmta_used', 'resources_assigned', 'ncpus sum'))
    fl.append(gen_field('cfree', 'free', {'hj': 'l', 'rj': 'r'}, 'fmta_free',
                        'resources_available resources_assigned', 'ncpus sum'))
    fl.append(gen_field('gpus', 'GPUs', {'hj': 'r', 'suppress': True},
                        'fmta_count', 'resources_available', 'ngpus sum'))
    fl.append(gen_field('gused', ['GPUs', 'used'],
                        {'hj': 'r', 'hs': '/', 'suppress': True},
                        'fmta_used', 'resources_assigned', 'ngpus sum'))
    fl.append(gen_field('gfree', ['', 'free'],
                        {'hj': 'l', 'rj': 'r', 'suppress': True},
                        'fmta_free', 'resources_available resources_assigned',
                        'ngpus sum'))
    fl.append(gen_field('mem', 'Mem', {'hj': 'r'}, 'fmta_mem',
                        'resources_available', 'mem sum_mem'))
    fl.append(gen_field('mused', ['Mem', 'used'], {'hj': 'r', 'hs': '/'},
                        'fmta_mem', 'resources_assigned', 'mem sum_mem'))
    fl.append(gen_field('mfree', 'free', {'hj': 'l', 'rj': 'r'}, 'fmta_mfree',
                        'resources_available resources_assigned',
                        'mem sum_mem'))
    fl.append(gen_field('state', 'State', None, 'fmt_by_name', 'state',
                        'const'))
    fl.append(gen_field('tasks', 'Tasks', {'hj': 'r'}, 'fmta_tcnt', 'jobs',
                        'sum'))
    fl.append(gen_field('jobs', 'Jobs', {'hj': 'r'}, 'fmta_jcnt', 'jobs',
                        'sum_jobs'))
    fl.append(gen_field('ninfo', 'Node Info', None, 'fmta_info',
                        'state comment resources_available partition '
                        'queue resv', 'const'))
    return fl


def gen_field_list_q():
    '''Generate field list for alternate queue display
    '''
    fl = []
    rc = {'hj': 'c'}
    rr = {'hj': 'r'}
    hcdr = {'hj': 'c', 'rj': 'r'}
    de = {'df': '-'}
    fl.append(gen_field('q_name', 'Queue', de, 'fmt_id', None))
    fl.append(gen_field('q_memory', 'Memory', hcdr, 'fmt_from_rsrc',
                        'resources_max', 'mem'))
    fl.append(gen_field('q_cput', 'CPU Time', rc, 'fmt_from_rsrc',
                        'resources_max', 'cput'))
    fl.append(gen_field('q_wtime', 'Walltime', rc, 'fmt_from_rsrc',
                        'resources_max', 'walltime'))
    fl.append(gen_field('q_nodect', 'Node', rr, 'fmt_from_rsrc',
                        'resources_max', 'nodect'))
    fl.append(gen_field('q_jrun', 'Run', rr, 'fmt_from_opt',
                        'state_count', 'running sum'))
    fl.append(gen_field('q_jqueue', 'Que', rr, 'fmt_from_opt',
                        'state_count', 'queued sum'))
    fl.append(gen_field('q_lm', 'Lm', rr, 'fmt_by_attr',
                        'max_running'))
    fl.append(gen_field('q_state', 'State', rr, 'fmt_short_state',
                        'enabled started'))
    return fl


def add_server_conn(smap, svr, conn):
    '''Add mappings from server to PBS connection

    Given a server name, look up its fqdn, IP addresses, aliases and
    add them to a mapping from server to connection.

    Args:
        smap = Dictionary mapping names to connections to be updated.
        svr = Server to add
        conn = Connection to associate with server
    Returns:
        None if name lookup failed, else True
    Exit:
        smap updated
    '''
    if svr in smap:
        # Don't replace existing entries with None
        if conn is None:
            return True
    try:
        (hname, aliases, ipaddrs) = socket.gethostbyname_ex(svr)
    except Exception:
        if svr not in smap:
            smap[svr] = None
        return None
    smap[svr] = conn
    smap[hname] = conn
    for x in aliases:
        smap[x] = conn
    for x in ipaddrs:
        smap[x] = conn
    return True


def check_W_bool(name, default=False):
    '''Get boolean value from opts_W

    Args:
        name = nmae of option
        default = value if option not present
    Returns:
        True if option present in opts_W and not set false
        default otherwise
    '''
    global opts_W
    namee = name + '='
    # Last match decides, so search backward
    for idx in range(len(opts_W) - 1, -1, -1):
        wopt = opts_W[idx]
        # Option by itself is True
        if wopt == name:
            return True
        # Examine name=value
        if not wopt.startswith(namee):
            continue
        val = wopt[len(namee):].lower()
        if val == 't' or val == 'true' or val == '1':
            return True
        if val == 'f' or val == 'false' or val == '0':
            return False
    return default


def check_W_int(name, default=0):
    '''Get integer value from opts_W

    Args:
        name = name of option
        default = value if option not present
    Returns:
        Integer value of found option
    '''
    global opts_W
    namee = name + '='
    # Last match decides, so search backward
    for idx in range(len(opts_W) - 1, -1, -1):
        wopt = opts_W[idx]
        # option by itself gives 1
        if wopt == name:
            return 1
        if not wopt.startswith(namee):
            continue
        # Examine name=value
        val = wopt[len(namee):].lower()
        try:
            return int(val)
        except ValueError:
            if val == '':
                return default
            if val == 't':
                return 1
            if val == 'f':
                return 0
    return default


def check_W_str(name, default=''):
    global opts_W
    namee = name + '='
    # Last match decides, so search backward
    for idx in range(len(opts_W) - 1, -1, -1):
        wopt = opts_W[idx]
        # Option by itself is default
        if wopt == name:
            return default
        # Examine name=value
        if not wopt.startswith(namee):
            continue
        val = wopt[len(namee):]
        return val
    return default


def condense_vnode_info(nodes):
    '''Sum info for vnodes into their natural vnodes

    That is, assign numeric attributes for the natural vnode to the sums
    of the attributes for the vnodes for the host.

    Args:
        nodes = list of node info dicts
    Returns:
        new version of the list
    '''
    def safeint(s):
        try:
            t = int(s)
        except ValueError:
            t = 0
        return t

    def safemem(s):
        '''Return a memory value as a float number of bytes'''
        if isinstance(s, float):
            return s
        t = unsuffix(s)
        return t

    def cmpf(item):
        '''Ugly function to supply to sorted() to sort vnode info

        The goal is for the node names to be sorted into human order,
        which basically means digit portions sort by numeric value
        rather than alphabetically. This routine takes the item id
        and returns a list of alternating strings and integers.
        Special handling is required for vnode names (abc[foo]).
        '''
        name = item['id']
        parts = list()
        flds = name.split('[', 1)
        first_pass = True
        for nm in flds:
            s = 0
            idx = 0
            nlen = len(nm)
            while idx < nlen:
                s = idx
                while idx < nlen:
                    if nm[idx].isdigit():
                        break
                    idx = idx + 1
                cpart = nm[s:idx]
                s = idx
                while idx < nlen:
                    if not nm[idx].isdigit():
                        break
                    idx = idx + 1
                npart = nm[s:idx]
                if npart != '':
                    npart = int(npart)
                else:
                    npart = -1
                parts.extend([cpart, npart])
            if first_pass and len(flds) > 1:
                parts.extend(['[', -1])
            first_pass = False
        return parts

    nat_info = None
    nat_name = None
    oddballs = list()
    new_list = list()
    ignore_states = set(['free'])
    merge_states = check_W_bool('merge_states')
    for ninfo in sorted(nodes, key=cmpf):
        cur_name = ninfo.get('resources_available.host', '').split('.')[0]
        if cur_name != nat_name:
            # New Mom, flush condensed node info to new list
            if nat_info:
                new_list.append(nat_info)
                new_list.extend(oddballs)
            nat_info = ninfo
            nat_name = cur_name
            oddballs = list()
            continue
        # A vnode, check if there is something odd about it
        odd = False
        while True:
            for a in ['queue', 'vntype']:
                if nat_info.get(a, None) != ninfo.get(a, None):
                    odd = True
                    break
            if odd:
                break
            a = 'state'
            nat = nat_info.get(a)
            cur = ninfo.get(a)
            if nat != cur:
                nats = set(nat.split(','))
                curs = set(cur.split(','))
                if merge_states:
                    nats |= curs
                    nats -= set(['free'])
                    nat = ','.join(sorted(nats))
                    nat_info[a] = nat
                else:
                    curs -= ignore_states
                    if not curs.issubset(nats):
                        odd = True
            break
        if odd:
            oddballs.append(ninfo)
            continue
        # Condense current vnode into natural node
        for a in ['resources_available', 'resources_assigned']:
            for r in ['mem', 'vmem']:
                z = a + '.' + r
                t = safemem(nat_info.get(z, 0.0)) + safemem(ninfo.get(z, 0.0))
                nat_info[z] = ensuffix(t, 'K')
            for r in ['ncpus', 'ngpus']:
                z = a + '.' + r
                t = safeint(nat_info.get(z, 0)) + safeint(ninfo.get(z, 0))
                nat_info[z] = str(t)
        for a in ['jobs']:
            nats = set(nat_info.get(a, '').split(','))
            curs = set(ninfo.get(a, '').split(','))
            nat = ','.join(sorted(nats | curs))
            nat_info[a] = nat
        # End of condensing
    if nat_info:
        new_list.append(nat_info)
        new_list.extend(oddballs)
    return new_list


def decode_state_count(counts):
    '''Extract count of queued and running jobs from state_count field

    Args:
        counts = state_count field
    Returns:
        Tuple with integer counts of queued and running jobs.
    '''
    queued = 0
    running = 0
    for one_count in counts.split():
        mo = re.match(r'([A-Z])\w+:(\d+)', one_count)
        if mo:
            tag = mo.group(1)
            cnt = int(mo.group(2))
            if tag in 'HQW':
                queued += cnt
            elif tag in 'RB':
                running += cnt
    return (queued, running)


def extract_job_info(server):
    '''Extract NAS-specific info from sorted jobs data

    Using the cached copy of the server's sortedjobs file, squirrel away
    the info for each job, into gNAS_job_data, after appending the index
    of the job in the sorted file (which represents the results of
    scheduler's job sort.

    The lines of interest have the form (tab-separated):

    jobid queue user share_name starve est_start priority ncpus

    Args:
        server = server name
    '''
    global gNAS_job_data, gshare_data

    sname = server.split('.')[0]
    t = gshare_data.get(sname, None)
    if not t:
        return
    idx = 0
    for line in t.split('\n'):
        if line.startswith('#') or line == '':
            # Skip comments and empty lines
            continue
        (jobid, rest) = line.split('\t', 1)
        jobid = jobid.strip()
        gNAS_job_data[jobid] = line + '\t' + str(idx)
        idx += 1


def extract_share_info(server):
    '''Extract share info from sortedjobs data

    We create a dict of dicts for each share entity listed
    Interesting lines have the format
    #A     entity=[class] gross net ncpus inuse leader
    Where entity is the mission name, or a group:user
        gross = allocated share from shares file
        net = gross adjusted by unavailable resources
        ncpus = units currently in use
        inuse = e+ed/l+ld/b+bd
            where e = inuse, but exempt
                l = inuse, subject to share limits
                b = inuse, borrowed from other shares
                d suffix = queued demands of each of the above
        leader = base mission of allocation
    The entry for root is special and reflects overall information.
    Entries with gross == -1 don't have their own allocation, but use
    the allocation given by their leader field.
    E.g.,
        #A              root=   520 0   2   0+0/0+0/0+0 root
        #A               SMD=   500 500 1   0+0/0+0/0+0 SMD
        #A               NAS=   20  20  0   0+0/0+0/1+5 NAS
        #A dtalcott:dtalcott=   -1  0   -1  0+0/0+0/1+5 NAS
    Args:
        server = name of server
    '''
    global gshare_entity_info, gshare_data, verbose

    gshare_entity_info = dict()
    sname = server.split('.')[0]
    t = gshare_data.get(sname, None)
    if not t:
        return
    idx = 0                 # Used later to sort
    for line in t.split('\n'):
        if not line.startswith('#A'):
            continue
        flds = line[2:].split('\t')
        if len(flds) != 6:
            if verbose:
                print("Malformed line in sortedjobs file:", line,
                      file=sys.stderr)
            continue
        z = flds[0].split('=')
        if len(z) != 2:
            if verbose:
                print("Malformed share entity in sortedjobs file:", line,
                      file=sys.stderr)
            continue
        entity = z[0].strip()
        cls = z[1].strip()
        key = entity if cls == '' else '<' + cls + '> ' + entity
        try:
            gross = int(flds[1])
            net = int(flds[2])
            alloc = int(flds[3])
            inuse = flds[4]
            leader = flds[5]
        except Exception:
            if verbose:
                print("Bad values in sortedjobs file:", line,
                      file=sys.stderr)
            continue
        mo = re.match(r'(\d+)\+(\d+)/(\d+)\+(\d+)/(\d+)\+(\d+)$', inuse)
        if not mo:
            if verbose:
                print("Bad usage in sortedjobs file:", line,
                      file=sys.stderr)
            continue
        e, ed, l, ld, b, bd = mo.groups()
        item = {'gross': gross, 'net': net, 'alloc': alloc,
                'e': int(e), 'ed': int(ed), 'l': int(l), 'ld': int(ld),
                'b': int(b), 'bd': int(bd),
                'leader': leader, 'idx': idx}
        idx += 1
        gshare_entity_info[key] = item
    return


new_section_called = False


def new_section():
    '''Output blank line

    Print a blank line except before the first call
    '''
    global new_section_called
    if new_section_called:
        print()
    new_section_called = True


def plug_job_info(bs):
    '''Add NAS-specific info to job info

    Args:
        bs = batch status from call to pbs_statjob, etc
    Exit:
        bs updated
    '''
    global gNAS_job_data

    for job in bs:
        jobid = job['id']
        if jobid in gNAS_job_data:
            (jid, queue, user, share, starve, est_start, pri, ncpus, idx) = \
                gNAS_job_data[jobid].split('\t')
            job['Priority'] = pri
            job['share_entity'] = share
            job['spri'] = int(idx)
    return


def pretty_state_counts(ilist):
    '''Convert state info into "pretty" output

    Args:
        ilist = List of dicts with 'state_count' attributes to be beautified
    Returns:
        A list of strings suitable for printing. The first element is a header
        and the remaining are state counts, one for each element in ilist.
    '''
    sclist = []
    maxsn = []
    for info in ilist:
        state_count = info.get('state_count', '')
        state_names = []
        state_cnts = []
        for one_count in state_count.split():
            mo = re.match(r'([A-Z])\w+:(\d+)', one_count)
            if mo:
                state_names.append(mo.group(1))
                state_cnts.append(mo.group(2))
        if len(state_names) > len(maxsn):
            maxsn = state_names
        sclist.append(state_cnts)
    # Deal with no data
    if len(maxsn) < 1:
        return ['--'] * (len(ilist) + 1)
    # Now, use layout to make into uniform size for all elements of ilist
    cfg = layout.Config()
    idx = 0
    for sn in maxsn:
        idx += 1
        s = '/' if idx < len(maxsn) else ''
        cfg.add_field(sn, hf='_', hj='r', hs=s, df='')
    scrows = layout.layout(cfg, sclist)
    return scrows


def summarize_nodes(flist, nlist):
    '''Produce one line summary of multiple nodes

    Args:
        fmtr = formatter used to create nlist entries
        nlist = list of node entries, where each entry is a list of
            field values
    '''
    def safeint(x):
        try:
            t = int(x)
        except Exception:
            t = 0
        return t
    # Initialize result row
    new_row = []
    for idx in range(len(flist)):
        # Summarize each field appropriately
        opt = flist[idx]['opt']
        if 'sum' in opt:
            total = 0
            for mom_row in nlist:
                total += safeint(mom_row[idx])
            fldval = str(total)
            if total == 0 and 'squash0' in opt:
                fldval = '--'
        elif 'sum_mem' in opt:
            total = 0
            for mom_row in nlist:
                t = mom_row[idx]
                total += unsuffix(t) if t != '--' else 0.0
            fldval = ensuffix(total)
        elif 'sum_jobs' in opt:
            total = 0
            for mom_row in nlist:
                total += safeint(mom_row[idx])
            # Because jobs might be running on multiple hosts, the
            # only totals that can be reported accurately are 0 and 1
            fldval = str(total) if total <= 1 else ">1"
        elif 'sum_hosts' in opt:
            t = [x[x.find('=') + 1:] for x in opt if x.startswith('unit=')]
            unit = 'host' if len(t) == 0 else t[0]
            hcount = len(nlist)
            fldval = ' ' + str(hcount) + ' ' + unit + \
                     ('s' if hcount != 1 else ' ')
        elif 'const' in opt:
            fldval = nlist[0][idx]
        else:
            fldval = '--'
        new_row.append(fldval)
    return new_row


if __name__ == '__main__':
    sys.exit(main())

# vi:ts=4:sw=4:expandtab
